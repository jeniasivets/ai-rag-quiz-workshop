{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0614425c-7f0f-4ecb-b8dc-f4c596488563",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Building Solutions with LLMs and Retrieval Augmented Generation (RAG): Introduction\n",
    "\n",
    "Welcome to this AI workshop! In this workshop we will get deeper into AI, RAG, chatbots, embeddings, evaluations and more.\n",
    "But first lets get used to this notebook enviroment.\n",
    "\n",
    "### Getting started with Jupyter Notebooks\n",
    "Notebooks are essential tools in the toolkit of data scientists and machine learning engineers. \n",
    "They facilitate interactive coding and effective visualization of results, making them invaluable for data exploration and analysis.\n",
    "Jupyter allows you to create cells that can contain either text or code. \n",
    "You can execute code cells by pressing Ctrl + Enter, enabling you to run your code and see results instantly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "1. Run the cell bellow a couple of times to get used to the notebook behaviour.\n",
    "2. Restart the kernel (Restart) button above and run again this code to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a if it does not exist\n",
    "if  'a' not in globals():\n",
    "    a = 42\n",
    "\n",
    "a = a+1\n",
    "\n",
    "# press ctrl+enter to run the cell, do that multiple times\n",
    "# notice how the variable `a` is persistent in the cell\n",
    "# you can run full programs in a notebook.\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## LLMs\n",
    "LLMs, or Large Language Models, are advanced AI systems designed to understand and generate human language. They use deep learning techniques and are trained on vast amounts of text data. Key features include their large scale (billions of parameters), ability to perform various language tasks (like translation and summarization), and contextual understanding. \n",
    "### MistralAI\n",
    "\n",
    "For this workshop we wil use [Mistral AI](https://mistral.ai/) langauge models, which are similar in concept to OpenAI's ChatGPT and Anthropic's Claude. Mistral AI offers both premier models and free models, with a strong emphasis on open-source availability. We are going to use [Mistral Small](https://docs.mistral.ai/getting-started/models/models_overview/#premier-models) model today. \n",
    "\n",
    "If your installation finalized correctly you should have mistral already installed. \n",
    "The command below checks if mistral is installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mistralai\n",
      "Version: 1.1.0\n",
      "Summary: Python Client SDK for the Mistral AI API.\n",
      "Home-page: https://github.com/mistralai/client-python.git\n",
      "Author: Mistral\n",
      "Author-email: \n",
      "License: \n",
      "Location: /home/vscode/.local/lib/python3.12/site-packages\n",
      "Requires: eval-type-backport, httpx, jsonpath-python, pydantic, python-dateutil, typing-inspect\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for some reason mistral did not installed successfully you can get it (and all other dependencies of this workshop by installing it directly with the command below), just uncomment the lines which start with \"!pip install\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/explodinggradients/ragas (from -r ../requirements.txt (line 10))\n",
      "  Cloning https://github.com/explodinggradients/ragas to /tmp/pip-req-build-fd4ra4l1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/explodinggradients/ragas /tmp/pip-req-build-fd4ra4l1\n",
      "  Resolved https://github.com/explodinggradients/ragas to commit cc31f65d4b7c7cd6bbf686b9073a0dfaacfbcbc5\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sentence-transformers~=3.2.0 (from -r ../requirements.txt (line 1))\n",
      "  Using cached sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas~=2.2.3 (from -r ../requirements.txt (line 2))\n",
      "  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting mistralai~=1.1.0 (from -r ../requirements.txt (line 3))\n",
      "  Using cached mistralai-1.1.0-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting watchdog~=5.0.3 (from -r ../requirements.txt (line 4))\n",
      "  Using cached watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting streamlit~=1.39.0 (from -r ../requirements.txt (line 5))\n",
      "  Using cached streamlit-1.39.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting python-dotenv (from -r ../requirements.txt (line 6))\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: ipykernel in /home/vscode/.local/lib/python3.12/site-packages (from -r ../requirements.txt (line 7)) (6.29.5)\n",
      "Collecting fire (from -r ../requirements.txt (line 8))\n",
      "  Using cached fire-0.7.0-py3-none-any.whl\n",
      "Collecting langchain_mistralai (from -r ../requirements.txt (line 9))\n",
      "  Using cached langchain_mistralai-0.2.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-huggingface (from -r ../requirements.txt (line 11))\n",
      "  Using cached langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting nltk (from -r ../requirements.txt (line 12))\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting rapidfuzz (from -r ../requirements.txt (line 13))\n",
      "  Using cached rapidfuzz-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting matplotlib (from -r ../requirements.txt (line 14))\n",
      "  Using cached matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting einops (from -r ../requirements.txt (line 15))\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting tqdm (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting scipy (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas~=2.2.3->-r ../requirements.txt (line 2))\n",
      "  Using cached numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vscode/.local/lib/python3.12/site-packages (from pandas~=2.2.3->-r ../requirements.txt (line 2)) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas~=2.2.3->-r ../requirements.txt (line 2))\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas~=2.2.3->-r ../requirements.txt (line 2))\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting eval-type-backport<0.3.0,>=0.2.0 (from mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached eval_type_backport-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpx<0.28.0,>=0.27.0 (from mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jsonpath-python<2.0.0,>=1.0.6 (from mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic<3.0.0,>=2.9.0 (from mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached pydantic-2.10.2-py3-none-any.whl.metadata (170 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas~=2.2.3->-r ../requirements.txt (line 2))\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting typing-inspect<0.10.0,>=0.9.0 (from mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/vscode/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas~=2.2.3->-r ../requirements.txt (line 2)) (1.16.0)\n",
      "Collecting altair<6,>=4.0 (from streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=20 in /home/vscode/.local/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (24.2)\n",
      "Collecting Pillow (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached pyarrow-18.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting requests<3,>=2.27 (from streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-extensions<5,>=4.3.0 (from streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (3.1.41)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /home/vscode/.local/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (6.4.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/vscode/.local/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/vscode/.local/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (1.8.9)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/vscode/.local/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (8.29.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/vscode/.local/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/vscode/.local/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/vscode/.local/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /home/vscode/.local/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (1.6.0)\n",
      "Requirement already satisfied: psutil in /home/vscode/.local/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (6.1.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/vscode/.local/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (26.2.0)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/vscode/.local/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (5.14.3)\n",
      "Collecting termcolor (from fire->-r ../requirements.txt (line 8))\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting httpx-sse<1,>=0.3.1 (from langchain_mistralai->-r ../requirements.txt (line 9))\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.15 (from langchain_mistralai->-r ../requirements.txt (line 9))\n",
      "  Using cached langchain_core-0.3.21-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting tokenizers<1,>=0.15.1 (from langchain_mistralai->-r ../requirements.txt (line 9))\n",
      "  Using cached tokenizers-0.20.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting datasets (from ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting tiktoken (from ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting langchain (from ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached langchain-0.3.8-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community (from ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain_openai (from ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached langchain_openai-0.2.10-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting appdirs (from ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting openai>1 (from ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached openai-1.55.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pysbd>=0.3.4 (from ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting joblib (from nltk->-r ../requirements.txt (line 12))\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk->-r ../requirements.txt (line 12))\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->-r ../requirements.txt (line 14))\n",
      "  Using cached contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->-r ../requirements.txt (line 14))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->-r ../requirements.txt (line 14))\n",
      "  Using cached fonttools-4.55.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (164 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->-r ../requirements.txt (line 14))\n",
      "  Using cached kiwisolver-1.4.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->-r ../requirements.txt (line 14))\n",
      "  Using cached pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting jinja2 (from altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached narwhals-1.14.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit~=1.39.0->-r ../requirements.txt (line 5)) (4.0.11)\n",
      "Collecting anyio (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting certifi (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sniffio (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.20.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: decorator in /home/vscode/.local/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/vscode/.local/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (0.19.2)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/vscode/.local/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/vscode/.local/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /home/vscode/.local/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/vscode/.local/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/vscode/.local/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r ../requirements.txt (line 7)) (4.3.6)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 9))\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.125 (from langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 9))\n",
      "  Using cached langsmith-0.1.146-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>1->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>1->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached jiter-0.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.9.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic<3.0.0,>=2.9.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.27->streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.27->streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (69.0.3)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<0.10.0,>=0.9.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached aiohttp-3.11.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached SQLAlchemy-2.0.36-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas~=2.2.3->-r ../requirements.txt (line 2))\n",
      "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached SQLAlchemy-2.0.35-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached propcache-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached yarl-1.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit~=1.39.0->-r ../requirements.txt (line 5)) (5.0.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/vscode/.local/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (0.8.4)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 9))\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached rpds_py-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 9))\n",
      "  Using cached orjson-3.10.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 9))\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit~=1.39.0->-r ../requirements.txt (line 5))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/vscode/.local/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/vscode/.local/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (0.2.13)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10))\n",
      "  Using cached greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/vscode/.local/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/vscode/.local/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/vscode/.local/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (0.2.3)\n",
      "Using cached sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached mistralai-1.1.0-py3-none-any.whl (229 kB)\n",
      "Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Using cached watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "Using cached streamlit-1.39.0-py2.py3-none-any.whl (8.7 MB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached langchain_mistralai-0.2.2-py3-none-any.whl (14 kB)\n",
      "Using cached langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached rapidfuzz-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached fonttools-4.55.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Using cached jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Using cached kiwisolver-1.4.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "Using cached langchain_core-0.3.21-py3-none-any.whl (409 kB)\n",
      "Using cached openai-1.55.1-py3-none-any.whl (389 kB)\n",
      "Using cached pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Using cached protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Using cached pyarrow-18.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "Using cached pydantic-2.10.2-py3-none-any.whl (456 kB)\n",
      "Using cached pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Using cached pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached tokenizers-0.20.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached langchain-0.3.8-py3-none-any.whl (1.0 MB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "Using cached langchain_community-0.3.8-py3-none-any.whl (2.4 MB)\n",
      "Using cached langchain_openai-0.2.10-py3-none-any.whl (50 kB)\n",
      "Using cached tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "Using cached scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
      "Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached aiohttp-3.11.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached jiter-0.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\n",
      "Using cached langsmith-0.1.146-py3-none-any.whl (311 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached narwhals-1.14.2-py3-none-any.whl (225 kB)\n",
      "Using cached pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "Using cached safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached SQLAlchemy-2.0.35-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Using cached greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (613 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Using cached orjson-3.10.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Using cached propcache-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (248 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached rpds_py-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (364 kB)\n",
      "Using cached yarl-1.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Building wheels for collected packages: ragas\n",
      "  Building wheel for ragas (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ragas: filename=ragas-0.2.7.dev2+gcc31f65-py3-none-any.whl size=155986 sha256=27b56fec24fd3fc88c0f579ed10212e5baae46d680da786b080dffe9e6561462\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-v01t8mcd/wheels/aa/db/38/a6cb74f407e90caf42004080da76661f852984de5b2ea8f5dd\n",
      "Successfully built ragas\n",
      "Installing collected packages: pytz, mpmath, appdirs, xxhash, watchdog, urllib3, tzdata, typing-extensions, tqdm, toml, threadpoolctl, termcolor, tenacity, sympy, sniffio, safetensors, rpds-py, regex, rapidfuzz, pyyaml, python-dotenv, python-dateutil, pysbd, pyparsing, pyarrow, protobuf, propcache, Pillow, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, narwhals, mypy-extensions, multidict, mdurl, marshmallow, MarkupSafe, kiwisolver, jsonpointer, jsonpath-python, joblib, jiter, idna, httpx-sse, h11, greenlet, fsspec, frozenlist, fonttools, filelock, eval-type-backport, einops, distro, dill, cycler, click, charset-normalizer, certifi, cachetools, blinker, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, triton, SQLAlchemy, scipy, requests, referencing, pydantic-core, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, multiprocess, markdown-it-py, jsonpatch, jinja2, httpcore, fire, contourpy, anyio, aiosignal, tiktoken, scikit-learn, rich, requests-toolbelt, pydeck, pydantic, nvidia-cusolver-cu12, matplotlib, jsonschema-specifications, huggingface-hub, httpx, dataclasses-json, aiohttp, torch, tokenizers, pydantic-settings, openai, mistralai, langsmith, jsonschema, transformers, langchain-core, datasets, altair, streamlit, sentence-transformers, langchain-text-splitters, langchain_openai, langchain_mistralai, langchain-huggingface, langchain, langchain-community, ragas\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-10.4.0 SQLAlchemy-2.0.35 aiohappyeyeballs-2.4.3 aiohttp-3.11.7 aiosignal-1.3.1 altair-5.5.0 annotated-types-0.7.0 anyio-4.6.2.post1 appdirs-1.4.4 attrs-24.2.0 blinker-1.9.0 cachetools-5.5.0 certifi-2024.8.30 charset-normalizer-3.4.0 click-8.1.7 contourpy-1.3.1 cycler-0.12.1 dataclasses-json-0.6.7 datasets-3.1.0 dill-0.3.8 distro-1.9.0 einops-0.8.0 eval-type-backport-0.2.0 filelock-3.16.1 fire-0.7.0 fonttools-4.55.0 frozenlist-1.5.0 fsspec-2024.9.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.27.2 httpx-sse-0.4.0 huggingface-hub-0.26.2 idna-3.10 jinja2-3.1.4 jiter-0.7.1 joblib-1.4.2 jsonpatch-1.33 jsonpath-python-1.0.6 jsonpointer-3.0.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 kiwisolver-1.4.7 langchain-0.3.8 langchain-community-0.3.8 langchain-core-0.3.21 langchain-huggingface-0.1.2 langchain-text-splitters-0.3.2 langchain_mistralai-0.2.2 langchain_openai-0.2.10 langsmith-0.1.146 markdown-it-py-3.0.0 marshmallow-3.23.1 matplotlib-3.9.2 mdurl-0.1.2 mistralai-1.1.0 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 mypy-extensions-1.0.0 narwhals-1.14.2 networkx-3.4.2 nltk-3.9.1 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 openai-1.55.1 orjson-3.10.12 pandas-2.2.3 propcache-0.2.0 protobuf-5.28.3 pyarrow-18.1.0 pydantic-2.10.2 pydantic-core-2.27.1 pydantic-settings-2.6.1 pydeck-0.9.1 pyparsing-3.2.0 pysbd-0.3.4 python-dateutil-2.8.2 python-dotenv-1.0.1 pytz-2024.2 pyyaml-6.0.2 ragas-0.2.7.dev2+gcc31f65 rapidfuzz-3.10.1 referencing-0.35.1 regex-2024.11.6 requests-2.32.3 requests-toolbelt-1.0.0 rich-13.9.4 rpds-py-0.21.0 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sentence-transformers-3.2.1 sniffio-1.3.1 streamlit-1.39.0 sympy-1.13.1 tenacity-9.0.0 termcolor-2.5.0 threadpoolctl-3.5.0 tiktoken-0.8.0 tokenizers-0.20.4 toml-0.10.2 torch-2.5.1 tqdm-4.67.1 transformers-4.46.3 triton-3.1.0 typing-extensions-4.12.2 typing-inspect-0.9.0 tzdata-2024.2 urllib3-2.2.3 watchdog-5.0.3 xxhash-3.5.0 yarl-1.18.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///workspaces/ai-rag-quiz-workshop\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: chat_solution\n",
      "  Building editable for chat_solution (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for chat_solution: filename=chat_solution-1.0.0-0.editable-py3-none-any.whl size=2798 sha256=d1603a2af30a791174f9df1d70bde8ec5c0f1736960c35e5bae884d302cb0084\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jkgr0hwq/wheels/a4/c6/d9/ae4576476be94afa1f5b82e4417e5583256c74d55dc9e6c651\n",
      "Successfully built chat_solution\n",
      "Installing collected packages: chat_solution\n",
      "Successfully installed chat_solution-1.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this command will install all the dependencies of the requirements.txt file\n",
    "# !pip install -r ../requirements.txt\n",
    "\n",
    "# this command will install the code of the project itsel\n",
    "# !pip install -e ../\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting access to Mistral LLMs\n",
    "\n",
    "LLMs are large in size and use a lot of computing resources. The most common way to use them is to run them in the cloud via network calls. For the sake of this workshop, we will use the cloud version as well, so we don't need to download large models. If you are interested in running LLMs locally, check out [ollama](https://github.com/ollama/ollama), arguably the most advanced project that does this.\n",
    "\n",
    "\n",
    "#### API key\n",
    "The second step is to get a Mistral API key. You can find some API keys we prepared for this workshop in this [sheet](https://docs.google.com/spreadsheets/d/1ZwTpkG6OOuVrOx8nzPmgai_7Hwpo8Kun7yZrOmg_5K4/edit?gid=0#gid=0). Get the key (please write your name next to it in the sheet so that people know it is taken) and write it to the .env file using the command below in Task 2.\n",
    "\n",
    "If you want to get your own API key, you can do it on the [website here](https://auth.mistral.ai/ui/registration).You will need to sign up using your email address and phone number and create a new API key [here]((https://console.mistral.ai/api-keys/)). If you are having any trouble, check out the video instructions on how to do it [here](https://drive.google.com/file/d/1mqwkX1BRvg_RMZJQHjtvKq31RjWZ9MdK/view)\n",
    "\n",
    "\n",
    "# Task 2\n",
    "\n",
    "Use the cell below to write your API key into the file so you can use Mistral. We write the variable into a file named .env. The .env files are a standard in Python to load information like API keys and passwords that should not stay with the code for security reasons. **Make sure to delete your key from the cell once you write it to the .env file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file .env\n",
    "\n",
    "mistral_api_key = ''\n",
    "with open('../.env', 'w') as f:\n",
    "    f.write(f'MISTRAL_API_KEY=\"{mistral_api_key}\"')\n",
    "\n",
    "# make sure to delete the API key from this cell before commiting the file so you dont save your key in the repo which is a security risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if it worked\n",
    "\n",
    "Run the command below to check if writing the key worked.  It will trigger an error if it did not.  You might need to restart the kernel if it does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables from /workspaces/ai-rag-quiz-workshop/.env\n",
      "If you reached this point is because it wrote a key in the right place :)\n"
     ]
    }
   ],
   "source": [
    "# we now reload the the configuration file and it should show your key\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "env_file = find_dotenv()\n",
    "print(f\"Loading environment variables from {env_file}\")\n",
    "load_dotenv(env_file)\n",
    "\n",
    "import os\n",
    "env = os.getenv('MISTRAL_API_KEY')\n",
    "\n",
    "if not env:\n",
    "    raise ValueError(\"The API key is not set. Please set the MISTRAL_API_KEY environment variable.\")\n",
    "if env == 'REPLACE WITH YOUR KEY':\n",
    "    raise ValueError(\"You did not repalce the value with the real key\")\n",
    "print(\"If you reached this point is because it wrote a key in the right place :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running mistral\n",
    "Let's run the code below to import Mistral and initialize the Mistral client: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! You can call me Assistant. I'm here to help answer your questions or have a friendly conversation. How about you? What's your name?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Retrieve the Mistral API key from the environment variables\n",
    "mistral_api_key = os.getenv('MISTRAL_API_KEY')\n",
    "\n",
    "# Initialize the Mistral client with the API key\n",
    "mistral_client = Mistral(api_key=mistral_api_key)\n",
    "\n",
    "# The model below is the specific model we want to use\n",
    "model_name = \"mistral-small-latest\"\n",
    "\n",
    "# The code below defines a function `call_mistral_model` that sends a message to a Mistral model and returns the model's response text.\n",
    "response = mistral_client.chat.complete(\n",
    "    model = model_name,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"hello! What is your name?\",\n",
    "        }\n",
    "        ]\n",
    "    )\n",
    "# Extract only the text from the response\n",
    "response_text = response.choices[0].message.content\n",
    "print(response_text)\n",
    "\n",
    "## not how the result resembles natural language. Its the exact same concept as chatgpt.\n",
    "## feel free to play around with the prompt and see how the results change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having all of this complex code to call the LLM, we can simplify it by moving this complexity to a Python script. From the code below onwards, we will replace our calls to the Mistral API with calls to the LargeLanguageModel class.\n",
    "\n",
    "Try it out and take a look at the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! You can call me Assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from chat_solution.llm import LargeLanguageModel\n",
    "\n",
    "# Initialize an instance of the LargeLanguageModel class\n",
    "llm = LargeLanguageModel()\n",
    "# Make a call to Mistral using the LargeLanguageModel class\n",
    "response = llm.call(\"hello! What is your name?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond calling the LLM, the class contains rate limiting handling logic. Rate limiting is a mechanism implemented by APIs to control the number of requests a user can make in a given time frame. This is done to prevent abuse so that one user doesn't consume the entire capacity and slow down the service. However, rate limiting can be annoying because it can interrupt your workflow and force you to wait before making more requests.\n",
    "\n",
    "To make things easier, we've implemented a rate limit error controller in the LargeLanguageModel class (see usage example below) that automatically adds sleep intervals between requests to avoid exceeding the rate limit. We will use the LargeLanguageModel class moving forward to make calls to Mistral AI. This class has the same logic as the examples we showed above but includes a mechanism to counter the rate limiting issue.    \n",
    "\n",
    "Have a look at the class by Ctrl+clicking on the LargeLanguageModel name to jump directly to its implementation.\n",
    "\n",
    "## Exploring the LLM\n",
    "Now that we have seen how to make a basic call to the Mistral model using the [LargeLanguageModel](../chat_solution/llm.py) class, let's try some more prompts to see how the model responds to different types of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Modifying Prompts\n",
    "Below are some example use cases of how to use an LLM such as Mistral. Play around with the prompts and see the results. Modify the prompts to see how the model's responses change. This will help you understand how to craft effective prompts and get the desired output from the model.\n",
    "\n",
    "Try to:\n",
    "- Ask different types of questions\n",
    "- Change the text for summarization or extraction (see examples 2 and 3 below)\n",
    "- Alter the style of the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Asking for Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! 42Berlin is a coding school that follows the innovative educational model pioneered by 42, a coding school founded in Paris by Xavier Niel, Nicolas Sadirac, and Florian Bucher. The school is known for its unique approach to teaching programming and software development. Here are some key aspects of 42Berlin:\n",
      "\n",
      "### Unique Educational Model\n",
      "1. **Peer-to-Peer Learning**: 42Berlin emphasizes peer-to-peer learning, where students help each other solve problems and learn new concepts. This collaborative environment is designed to foster a strong sense of community and mutual support.\n",
      "\n",
      "2. **Project-Based Learning**: The curriculum is heavily project-based. Students work on real-world projects that help them apply theoretical knowledge in practical scenarios. This hands-on approach is intended to make learning more engaging and effective.\n",
      "\n",
      "3. **No Traditional Classes or Teachers**: There are no traditional lectures or teachers in the conventional sense. Instead, students learn through a combination of online resources, projects, and peer interactions. Mentors and staff are available to guide and support students, but the primary learning happens through self-study and collaboration.\n",
      "\n",
      "### Admission Process\n",
      "1. **Piscine (Pool)**: The admission process involves a rigorous, four-week intensive program called the \"Piscine.\" During this period, applicants are immersed in a challenging coding environment to test their problem-solving skills, resilience, and ability to learn quickly.\n",
      "\n",
      "2. **No Prerequisites**: 42Berlin does not require any formal educational background or prior programming experience. The school believes in evaluating candidates based on their potential and ability to learn rather than their past academic achievements.\n",
      "\n",
      "### Curriculum\n",
      "1. **Core Programming Skills**: The curriculum covers a wide range of programming languages and technologies, including C, C++, Python, web development, and more.\n",
      "\n",
      "2. **Soft Skills**: In addition to technical skills, the program focuses on developing soft skills such as teamwork, communication, and project management.\n",
      "\n",
      "3. **Flexible Learning Path**: Students can progress at their own pace, allowing for a personalized learning experience. This flexibility caters to different learning styles and speeds.\n",
      "\n",
      "### Community and Network\n",
      "1. **Global Network**: As part of the 42 network, students at 42Berlin have access to a global community of learners and alumni. This network can be invaluable for career opportunities and ongoing learning.\n",
      "\n",
      "2. **Events and Workshops**: The school hosts various events, workshops, and hackathons to keep students engaged and connected with the tech industry.\n",
      "\n",
      "### Career Prospects\n",
      "1. **Industry Connections**: 42Berlin has strong ties with the tech industry, which can lead to internships and job opportunities for graduates.\n",
      "\n",
      "2. **Alumni Success**: Many graduates of the 42 network have gone on to successful careers in tech, working for top companies or starting their own ventures.\n",
      "\n",
      "### Location\n",
      "42Berlin is located in the heart of Berlin, a city known for its vibrant tech scene and startup culture. This environment provides ample opportunities for networking, collaboration, and real-world experience.\n",
      "\n",
      "Overall, 42Berlin offers a distinctive and immersive learning experience designed to prepare students for successful careers in the tech industry.\n"
     ]
    }
   ],
   "source": [
    "# Example: Asking for information\n",
    "prompt = \"Can you tell me about coding school 42Berlin?\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Summarizing a Given Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42Berlin is a tuition-free coding school empowering the next generation of coders through accessible and inclusive tech education.\n"
     ]
    }
   ],
   "source": [
    "# Change the text into something else to see the results\n",
    "text_to_summarize = (\n",
    "    \"\"\"\n",
    "    42Berlin is a non-profit coding school offering software engineering education completely tuition free. \n",
    "    By making tech education more accessible and inclusive, they empower the next generation of coders.\n",
    "    Founded in 2021 and based in central Neukölln, we train our students up to the equivalent of Master’s level \n",
    "    and implement peer-learning methodologies that give autonomy to each student.\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "prompt = f\"Summarize the following text in one brief sentence: {text_to_summarize}\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: Extracting Information from a Given Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The year 42Berlin was founded is 2021.\n"
     ]
    }
   ],
   "source": [
    "# Change the text into something else to see the results\n",
    "text_to_extract_from = (\n",
    "    \"\"\"\n",
    "    42Berlin is a non-profit coding school offering software engineering education completely tuition free. \n",
    "    By making tech education more accessible and inclusive, they empower the next generation of coders.\n",
    "    Founded in 2021 and based in central Neukölln, we train our students up to the equivalent of Master’s level \n",
    "    and implement peer-learning methodologies that give autonomy to each student.\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "prompt = f\"Extract the year 42Berlin was founded from the following text: {text_to_extract_from}\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 3: Doing Maths -> When will the LLM make a mistake?\n",
    "\n",
    "LLMs are trained on text language so they are not necessarily good with maths. We can be confident that it will fail to produce the correct result if you ask a question that is complex enough. On the notebook below we increase the complexity of the task at every run. Run it until you see it diverging.\n",
    "\n",
    "What was the number of iterations necessary to make it break?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct result:  4\n",
      "Mistral result:  The result of 2 ** 2 is 4. In Python, the `**` operator is used for exponentiation, so 2 raised to the power of 2 is 4.\n"
     ]
    }
   ],
   "source": [
    "if 'a' not in globals() or 'b' not in globals():\n",
    "    a = 1\n",
    "    b = 1\n",
    "a = a + 1\n",
    "b = b + 1\n",
    "\n",
    "## ** in python is the power operator meaning: a to the power of b\n",
    "print(\"Correct result: \", a**b)\n",
    "response = llm.call(f\"what is the results of {a} ** {b}\")\n",
    "print(\"Mistral result: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably noted, mistral does not only return the results, it also explains it step by step. For example:\n",
    "\n",
    "```\n",
    "The result of 4 raised to the power of 4 (4 ** 4) is calculated as:\n",
    "4 * 4 * 4 * 4 = 256\n",
    "```\n",
    "\n",
    "Mistral and other language models deliberatelly add this longer explanations as they help the model commit less mistakes. This pattern is also known as chain of thought reasonsing. And is one of the most robust ways to improve LLM's responses. When you write prompts think about how to explain the steps of the reasonsing to improve the performance of your propmts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucination\n",
    "LLMs sometimes generate responses that are plausible-sounding but factually incorrect or nonsensical. This phenomenon is known as \"hallucination\".\n",
    "<br><br>\n",
    "***Hallucination*** can occur because the model generates text based on patterns in the training data rather than actual knowledge or retrieval of relevant information.\n",
    "LLMs will produce the most likelly words that they would find in similar text, they have no clue about fact vs fiction. They can confidently produce writing about things they have no clue about.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Demonstrating Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will ask the model a question that it might to hallucinate an answer for, showing the limitations of relying solely on language generation without retrieval.\n",
    "\n",
    "Try running the command below a few times in a row and see how the response by the LLM changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response likely to hallucinate:\n",
      "\n",
      "The workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech was named \"MLOps for Women.\" This workshop aimed to empower women in the field of machine learning operations (MLOps) by providing them with practical skills, knowledge, and networking opportunities.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question likely to cause hallucination\n",
    "prompt = \"What was the name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech?\"\n",
    "response = llm.call(prompt)\n",
    "print(\"Response likely to hallucinate:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's move on to the next section on Retrieval-Augmented Generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique that incorporates external information to a LLM to generate better responses. \n",
    "\n",
    "In RAG, a retrieval component searches and retrieves relevant information from a knowledge base or external documents, and a generation component uses this information to generate responses.\n",
    "This approach allows the model to access up-to-date information and provide more detailed and accurate answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Simple RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will demonstrate a simple example of how to use Retrieval-Augmented Generation. We will use a predefined set of documents, retrieve relevant information based on a query, and then generate a response using the retrieved information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_prompt(message: str, context: str):\n",
    "    \"\"\"\n",
    "    Message is the question that the user is asking.\n",
    "    Context is the information that we want to use to answer the question.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Answer the question only using the provided content.\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        User Question: {message}\n",
    "\n",
    "        Be helpful and friendly. If the information cannot be found respond with \"I don't know\"\n",
    "        \"\"\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the code in the cell below, you can compare how our LLM responses differ by the information that you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " The workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech was named \"MLOps for Beginners.\" This workshop aimed to introduce participants to the fundamentals of MLOps (Machine Learning Operations) and provide them with practical skills and knowledge to implement and manage machine learning projects effectively.\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " The name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech is \"AI Launchpad - Building Your First ML Pipeline.\"\n"
     ]
    }
   ],
   "source": [
    "message = \"What was the name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech?\"\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "# The workshop the MLOps Community hosted together with Girls in Tech Germany was called \"AI Launchpad: Building Your First Ml Pipeline\" or simply \"Building Your First ML Pipeline\"\n",
    "# We copy paste the info from our Eventbrite event page from the previous workshop and use this as context for the model to retrieve the right info from\n",
    "context = \"\"\"Title: AI Launchpad - Building Your First ML Pipeline: \n",
    "On Wednesday, June 5th, 2024, the MLOps Community Berlin in collaboration with Girls in Tech Germany hosted an interactive workshop for beginners who want to kick start their career in AI/ML. \n",
    "The workshop starts at 18.00h at 42Berlin. \n",
    "\n",
    "🔍 Why Attend?\n",
    "\n",
    "Gain hands-on experience building your first ML pipeline in an agile way\n",
    "Apply the fundamentals of statistical modeling and basic Python\n",
    "Opportunities to improve your portfolio \n",
    "Connect with ML professionals at different levels of seniority\n",
    "\n",
    "\n",
    "✨ The Agenda: \n",
    "\n",
    "6:00 pm - Arrive & Pizza \n",
    "6:30 pm -  Introduction MLOps and GiT\n",
    "6:45 pm - Workshop Introduction\n",
    "7:30 pm - Break\n",
    "7:45 pm - Workshop\n",
    "9:45 pm - Networking\n",
    "\n",
    "\n",
    "🎉 Highlights:\n",
    "\n",
    "Food and drinks provided\n",
    "Engaging discussions and networking opportunities\n",
    "Bring your laptop and get ready to learn!\n",
    "\n",
    "\n",
    "💼 Who Should Attend?\n",
    "\n",
    "Individuals starting their career in Machine Learning or Artificial Intelligence\n",
    "Those looking to transition into the field of AI/ML\n",
    "Anyone interested in contributing to and learning from the ML community\n",
    "Don't miss out on this chance to gain practical AI/ML skills while expanding your professional network! \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " I'm an assistant that operates solely on the data it has been trained on up until 2021, and I don't have real-time or future weather data. Therefore, I can't provide the weather forecast for Berlin on the 10th of December, 2027. For the most accurate and up-to-date weather information, I recommend checking a reliable weather website or application closer to the date.\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " On the 10th of December, 2027, the weather in Berlin is expected to be around 10 degrees Celsius.\n"
     ]
    }
   ],
   "source": [
    "# Let's try the same thing with Berlin weather data!\n",
    "context = \"\"\"\n",
    "The weather in Berlin  December of 2027 will be around 13 degrees Celsius.\n",
    "Specific dates:\n",
    "- 10th of December: 10 degrees Celsius\n",
    "- 15th of December: 15 degrees Celsius\n",
    "- 20th of December: 7 degrees Celsius\n",
    "\"\"\"\n",
    "\n",
    "message = \"What will be the weather in Berlin on the 10th of December of 2027?\"\n",
    "\n",
    "\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "Now try it yourself! Can you find some content on the internet (think, for example, recent news articles or very specific, locally relevant information that the LLM normally would not have access to). \n",
    "\n",
    "Play around with it and let the creative juices flow. Can you discover some more use cases for which you can use RAG can help make our LLM smarter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "YOUR CONTEXT HERE\n",
    "\"\"\" \n",
    "\n",
    "message = \"\"\"\n",
    "YOUR MESSAGE HERE\n",
    "\"\"\" \n",
    "\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "If you want to make your prompting even more advanced, you can look into Prompt Engineering best practices. <br><br>\n",
    "***Prompt Engineering*** is the process of designing and refining LLM prompts. It involves crafting questions, instructions, or statements that guide the model to produce desired outputs. Effective prompt engineering can significantly enhance the quality, relevance, and accuracy of the responses generated by the model.  \n",
    "\n",
    "Best practices change over time and are somewhat different depending on the LLM type, but general guidelines are:\n",
    "\n",
    "* **Clarity and Specificity:** Clearly state what you want the model to do. Avoid ambiguous language.\n",
    "* **Context and Detail:** Provide sufficient context to guide the model.\n",
    "* **Iterative Refinement:** Experiment with different phrasings and structures to see what works best.\n",
    "* **Constraints and Instructions:** Specify any constraints or formats you want the response to follow.\n",
    "* **Examples and Templates:** Use examples to show the model what kind of response you expect.\n",
    "* **Step-by-Step Reasoning:** Instead of providing a direct answer, aske the model to explain its thought process step-by-step. \n",
    "* **Breaking Down Complex Tasks:** For tasks that require multiple stages of reasoning or involve complex information, breaking them down into smaller, separated steps can lead to better results.\n",
    "\n",
    "You can learn more about prompt engineering [here](https://www.promptingguide.ai/). \n",
    "Some very useful examples of best practices for OpenAI GPT models can be found [here](https://platform.openai.com/docs/guides/prompt-engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it! \n",
    "\n",
    "RAGs enrich the prompt with additional information about the topic to generate responses. The external information can come from various sources, such as PDFs, Google search results, social media posts, and more. With that, we’ve built a simple Q&A RAG.\n",
    "\n",
    "## ===> Now head to notebook 2 on embedddings and how llms undertand language\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1848951197487297,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Steven Test Playground",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
